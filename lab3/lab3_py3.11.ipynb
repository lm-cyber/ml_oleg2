{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6742e81-d264-4e7c-943c-b5ec70643e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best.onnx  best_openvino_model\tbest.pt  last.pt\n"
     ]
    }
   ],
   "source": [
    "!ls ../detect4/train/weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca9bc9d7-9f4a-4727-8a2d-3ef8cfb2108f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r ../detect4/train/weights/best.onnx best.onnx\n",
    "!cp -r ../detect4/train/weights/best.pt best.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "681c4898-4e6a-43ac-8da8-20c2b730098c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yolo task=detect mode=export model=best.pt format=onnx opset=13 # write  in conslole "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91cdcf79-21ba-4d00-b8a4-7558b9f59534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94a6ce7e-23f0-11e9-a7f5-e30a5f40fd48.jpg\n",
      "94a73580-23f0-11e9-a924-3b95c1401421.jpg\n",
      "94a75df8-23f0-11e9-a99c-b70589e9b3f5.jpg\n",
      "94a7af88-23f0-11e9-aa81-5b7b64aebd80.jpg\n",
      "94a8280a-23f0-11e9-abca-a32c05d084e8.jpg\n",
      "94a8f348-23f0-11e9-adfa-939d45dcfc64.jpg\n",
      "94a951a8-23f0-11e9-aed9-475a4d94da1c.jpg\n",
      "94aa4810-23f0-11e9-b112-23992b646aea.jpg\n",
      "94aa6e3a-23f0-11e9-b166-03251cbcec9b.jpg\n",
      "94aa91bc-23f0-11e9-b1bf-7f297b452169.jpg\n",
      "ls: write error: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!ls ../annotations_s/images/val | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7b64e07-cce3-4255-84c5-40eb3e931563",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 07:53:14 deepsparse.pipeline WARNING  Could not create v2 'yolov8' pipeline, trying legacy\n",
      "DeepSparse, Copyright 2021-present / Neuralmagic, Inc. version: 1.8.0 COMMUNITY | (e3778e93) (release) (optimized) (system=avx2, binary=avx2)\n"
     ]
    }
   ],
   "source": [
    "from deepsparse import Pipeline\n",
    "\n",
    "# Specify the path to your YOLO11 ONNX model\n",
    "model_path = \"best.onnx\"\n",
    "\n",
    "# Set up the DeepSparse Pipeline\n",
    "yolo_pipeline = Pipeline.create(task=\"yolov8\", model_path=model_path)\n",
    "\n",
    "# Run the model on your images\n",
    "images = [\"../annotations_s/images/val/94a6ce7e-23f0-11e9-a7f5-e30a5f40fd48.jpg\"]\n",
    "pipeline_outputs = yolo_pipeline(images=images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96ee89a7-0be0-48a6-8369-3067cb1656eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YOLOOutput(boxes=[[[330.73236083984375, 298.5351333618164, 473.545654296875, 450.23229217529297], [24.9356689453125, 413.8678894042969, 164.16455078125, 520.8174591064453], [281.3670349121094, 21.91717529296875, 389.4460144042969, 149.87598609924316], [427.2222900390625, 24.362783432006836, 535.7445068359375, 236.3781280517578], [466.8013916015625, 452.6953582763672, 624.87451171875, 503.9292755126953], [90.71358489990234, 495.9767532348633, 201.91531372070312, 633.8363571166992]]], scores=[[0.8691822290420532, 0.7449894547462463, 0.7214380502700806, 0.7087214589118958, 0.6561912894248962, 0.635062038898468]], labels=[[15.0, 15.0, 15.0, 15.0, 26.0, 26.0]], intermediate_outputs=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43c6e27e-e11d-47bd-b17a-73f000116c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import time\n",
    "from PIL import Image\n",
    "import torch\n",
    "path_img  = \"../annotations_s/images/val/94a6ce7e-23f0-11e9-a7f5-e30a5f40fd48.jpg\"\n",
    "def test_pt(model_test , path=path_img,it=20):\n",
    "    torch.cpu.synchronize()\n",
    "    img = Image.open(path_img)\n",
    "    #warm up \n",
    "    for _ in range(5):\n",
    "        model_test.predict(img,verbose=False)\n",
    "        torch.cpu.synchronize()\n",
    "\n",
    "        \n",
    "    starttime = time.perf_counter()\n",
    "    for _ in range(it):\n",
    "        model_test.predict(img,verbose=False)\n",
    "        torch.cpu.synchronize()\n",
    "\n",
    "    return timedelta(seconds=time.perf_counter()-starttime)/it\n",
    "import numpy as np  \n",
    "def test_deepsparse(model_test , path=path_img,it=20):\n",
    "    #warm up \n",
    "    img = np.array(Image.open(path_img))\n",
    "\n",
    "    for _ in range(5):\n",
    "        model_test(images=[img],verbose=False)\n",
    "        torch.cpu.synchronize()\n",
    "\n",
    "        \n",
    "    starttime = time.perf_counter()\n",
    "    for _ in range(it):\n",
    "        model_test(images=[img],verbose=False)\n",
    "\n",
    "    return timedelta(seconds=time.perf_counter()-starttime)/it\n",
    "    \n",
    "def test_yolo(model_test , path=path_img,it=20):\n",
    "    #warm up \n",
    "    img = Image.open(path_img)\n",
    "\n",
    "    for _ in range(5):\n",
    "        model_test.predict(img,verbose=False)\n",
    "        torch.cpu.synchronize()\n",
    "\n",
    "        \n",
    "    starttime = time.perf_counter()\n",
    "    for _ in range(it):\n",
    "        model_test.predict(img,verbose=False)\n",
    "\n",
    "    return timedelta(seconds=time.perf_counter()-starttime)/it\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42350ed8-08df-4ecf-8b99-726903b6a7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/void/allinfo_new/venv3.11/venv/lib/python3.11/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(file, map_location='cpu'), file  # load\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"best.pt\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15e5c168-7ca3-48c4-83a0-a27c0e726e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.124 🚀 Python-3.11.10 torch-2.5.1+cpu CPU\n",
      "Model summary (fused): 268 layers, 68156310 parameters, 0 gradients\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from best.pt with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 38, 8400) (130.4 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.14.1 opset 19...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 2.2s, saved as best.onnx (260.2 MB)\n",
      "\n",
      "Export complete (4.5s)\n",
      "Results saved to \u001b[1m/home/void/ml_oleg/lab3\u001b[0m\n",
      "Predict:         yolo predict task=detect model=best.onnx imgsz=640 \n",
      "Validate:        yolo val task=detect model=best.onnx imgsz=640 data=test.yaml \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'best.onnx'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.export(format='onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d80111b2-f3dc-43b8-b86c-edc45d049a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv best.onnx best_opset19.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c1f524a-be35-4151-958e-e1542b21e554",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/void/allinfo_new/venv3.11/venv/lib/python3.11/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(file, map_location='cpu'), file  # load\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "datetime.timedelta(microseconds=390730)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"best.pt\")  \n",
    "test_pt(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8736f4c7-4142-4ba4-8e8a-3ce6cfffe2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_deepsparse = test_deepsparse(yolo_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce682006-9bdc-4146-b71d-f75982ceaf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_str=''' \tmodel \tmetrics/precision(B) \tmetrics/recall(B) \tmetrics/mAP50(B) \tmetrics/mAP50-95(B) \ttime \tnorm_time \tmetrics/precision(B)_norm \tmetrics/recall(B)_norm \tmetrics/mAP50(B)_norm \tmetrics/mAP50-95(B)_norm\n",
    "0 \ttorch \t0.619810 \t0.436514 \t0.506129 \t0.353711 \t652432 \t0.819392 \t1.033923 \t1.233787 \t1.291715 \t1.317634\n",
    "1 \tdq_torch \t0.599474 \t0.403165 \t0.476198 \t0.332022 \t796239 \t1.000000 \t1.000000 \t1.139528 \t1.215327 \t1.236839\n",
    "2 \tsq_onnx \t0.646825 \t0.353800 \t0.391827 \t0.268444 \t324608 \t0.407677 \t1.078988 \t1.000000 \t1.000000 \t1.000000\n",
    "3 \tdq_onnx \t0.719761 \t0.359985 \t0.480005 \t0.333922 \t727462 \t0.913623 \t1.200654 \t1.017482 \t1.225043 \t1.243917\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1c11631-165f-4371-9342-4628364e769c",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = table_str.split()[:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4b024cb-e144-4d79-94fe-ef6e60e1bcce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "rows = table_str.split()[11:]\n",
    "print(len(rows)/12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "759085d3-7f39-4db7-9321-c444fd0266eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_data = []\n",
    "for i in range(4):\n",
    "    rows_data.append(rows[i*12+1:(i+1)*12])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a98abe67-4494-4a34-a005-21bd94499dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "conwert_data = defaultdict(list)\n",
    "for items in rows_data:\n",
    "    for name,item in zip(columns,items):\n",
    "        conwert_data[name].append(item)\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9794823c-2367-4606-829f-fa7907b22c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': ['torch', 'dq_torch', 'sq_onnx', 'dq_onnx'],\n",
       " 'metrics/precision(B)': ['0.619810', '0.599474', '0.646825', '0.719761'],\n",
       " 'metrics/recall(B)': ['0.436514', '0.403165', '0.353800', '0.359985'],\n",
       " 'metrics/mAP50(B)': ['0.506129', '0.476198', '0.391827', '0.480005'],\n",
       " 'metrics/mAP50-95(B)': ['0.353711', '0.332022', '0.268444', '0.333922'],\n",
       " 'time': ['652432', '796239', '324608', '727462'],\n",
       " 'norm_time': ['0.819392', '1.000000', '0.407677', '0.913623'],\n",
       " 'metrics/precision(B)_norm': ['1.033923', '1.000000', '1.078988', '1.200654'],\n",
       " 'metrics/recall(B)_norm': ['1.233787', '1.139528', '1.000000', '1.017482'],\n",
       " 'metrics/mAP50(B)_norm': ['1.291715', '1.215327', '1.000000', '1.225043'],\n",
       " 'metrics/mAP50-95(B)_norm': ['1.317634', '1.236839', '1.000000', '1.243917']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(conwert_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7ce7d4b-1a48-4d66-ad9b-c3742e4aeb97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>metrics/precision(B)</th>\n",
       "      <th>metrics/recall(B)</th>\n",
       "      <th>metrics/mAP50(B)</th>\n",
       "      <th>metrics/mAP50-95(B)</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>torch</td>\n",
       "      <td>0.619810</td>\n",
       "      <td>0.436514</td>\n",
       "      <td>0.506129</td>\n",
       "      <td>0.353711</td>\n",
       "      <td>652432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dq_torch</td>\n",
       "      <td>0.599474</td>\n",
       "      <td>0.403165</td>\n",
       "      <td>0.476198</td>\n",
       "      <td>0.332022</td>\n",
       "      <td>796239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sq_onnx</td>\n",
       "      <td>0.646825</td>\n",
       "      <td>0.353800</td>\n",
       "      <td>0.391827</td>\n",
       "      <td>0.268444</td>\n",
       "      <td>324608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dq_onnx</td>\n",
       "      <td>0.719761</td>\n",
       "      <td>0.359985</td>\n",
       "      <td>0.480005</td>\n",
       "      <td>0.333922</td>\n",
       "      <td>727462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model metrics/precision(B) metrics/recall(B) metrics/mAP50(B)  \\\n",
       "0     torch             0.619810          0.436514         0.506129   \n",
       "1  dq_torch             0.599474          0.403165         0.476198   \n",
       "2   sq_onnx             0.646825          0.353800         0.391827   \n",
       "3   dq_onnx             0.719761          0.359985         0.480005   \n",
       "\n",
       "  metrics/mAP50-95(B)    time  \n",
       "0            0.353711  652432  \n",
       "1            0.332022  796239  \n",
       "2            0.268444  324608  \n",
       "3            0.333922  727462  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(conwert_data)\n",
    "df = df.drop(['norm_time','metrics/precision(B)_norm','metrics/recall(B)_norm','metrics/mAP50(B)_norm','metrics/mAP50-95(B)_norm'],axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9e349bb-0402-4763-b126-f407c645f686",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.124 🚀 Python-3.11.10 torch-2.5.1+cpu CPU\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from best.pt with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 38, 8400) (130.4 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.14.1 opset 19...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 2.5s, saved as best.onnx (260.2 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mOpenVINO:\u001b[0m starting export with openvino 2024.5.0-17288-7975fa5da0c-refs/pull/3856/head...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ INFO ] MO command line tool is considered as the legacy conversion API as of OpenVINO 2023.2 release.\n",
      "In 2025.0 MO command line tool and openvino.tools.mo.convert_model() will be removed. Please use OpenVINO Model Converter (OVC) or openvino.convert_model(). OVC represents a lightweight alternative of MO and provides simplified model conversion API. \n",
      "Find more information about transition from MO to OVC at https://docs.openvino.ai/2023.2/openvino_docs_OV_Converter_UG_prepare_model_convert_model_MO_OVC_transition.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mOpenVINO:\u001b[0m export success ✅ 2.0s, saved as best_openvino_model/ (260.4 MB)\n",
      "\n",
      "Export complete (6.2s)\n",
      "Results saved to \u001b[1m/home/void/ml_oleg/lab3\u001b[0m\n",
      "Predict:         yolo predict task=detect model=best_openvino_model imgsz=640 \n",
      "Validate:        yolo val task=detect model=best_openvino_model imgsz=640 data=test.yaml \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'best_openvino_model'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.export(format = 'openvino')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "613c92b0-33f3-4a30-9a3e-f1d2f35a98d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify', or 'pose'.\n"
     ]
    }
   ],
   "source": [
    "openvino_model = YOLO('best_openvino_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "611c59e9-72ca-42ec-b72d-1257a66c380f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.124 🚀 Python-3.11.10 torch-2.5.1+cpu CPU\n",
      "Loading best_openvino_model for OpenVINO inference...\n",
      "Forcing batch=1 square inference (1,3,640,640) for non-PyTorch models\n",
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fc3d886c-5952-11ec-bcdb-d7519ad21bc8.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fc3ea224-5952-11ec-bf0a-2f275af30bd6.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fc3ea814-5952-11ec-bf18-4f1e8f881a41.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fc3ede2e-5952-11ec-bf9d-a3e63cb46d1c.jpg: 4 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fc426d32-5952-11ec-8b26-efc80d05c8c1.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fc427476-5952-11ec-8b31-f331aa9edd11.jpg: 4 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fc427a70-5952-11ec-8b40-07f33991eacd.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fc4470aa-5952-11ec-8f77-57ac9b749a59.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fc45d3c8-5952-11ec-9071-c746cf5ca73c.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fc467b52-5952-11ec-9177-a32b57720073.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fc47734a-5952-11ec-9262-13f8ef139327.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fc485a58-5952-11ec-9367-d7073ffd5cb7.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fc4a2a18-5952-11ec-94db-bbfefa31a79d.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fc4b1932-5952-11ec-9634-03b9c138b374.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fc987088-5952-11ec-bf6d-93eb0e1eb8a9.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fc98823a-5952-11ec-bf80-7fdf6c9177ba.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fc9a49d0-5952-11ec-b757-ffef9f6c7f71.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fc9c78cc-5952-11ec-ba22-338333c5b7e4.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fc9e99fe-5952-11ec-bcb2-1fd752b9f109.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fca5a69a-5952-11ec-bdb4-43e170a96e52.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fca64e1a-5952-11ec-be7c-7b3a54b135eb.jpg: 3 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fca840bc-5952-11ec-9692-8bc82d5d62c4.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fca87424-5952-11ec-96d2-471bf60272df.jpg: 3 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fcaa693c-5952-11ec-9903-438f68af897a.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fcabd790-5952-11ec-9ad1-23afcf4b9cc1.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fcabddd0-5952-11ec-9ae1-bf12db9dbce3.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fcaf16b2-5952-11ec-9ebc-0f7193d4aef5.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fcafba9a-5952-11ec-9fa0-b7fa6a42b218.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fcb21e52-5952-11ec-a2a0-3ba6571de549.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fcb39df4-5952-11ec-a47f-f341415327f4.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fcb5e7c6-5952-11ec-a750-d350ec5470ed.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fcb606e8-5952-11ec-a770-0388bdcd34c2.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fcb63f50-5952-11ec-a7be-d3c966f1d793.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fcbed94e-5952-11ec-b2b7-ab097c23f062.jpg: 3 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fcc0edec-5952-11ec-b572-dfb09aedd06b.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fcc9bc2e-5952-11ec-b852-27b40cf0a87c.jpg: 5 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fcd1b79e-5952-11ec-ba16-733db70ce958.jpg: 5 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fcd8333a-5952-11ec-bfe7-d718267f2d8e.jpg: 4 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fcd95378-5952-11ec-a256-9f5ea652c3cc.jpg: 3 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fcda9c92-5952-11ec-a3af-6fa41dbaea94.jpg: 5 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fcdda95a-5952-11ec-a736-efbc13520e8f.jpg: 3 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fcde19e4-5952-11ec-a7ab-e72745545c8c.jpg: 5 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fcde305a-5952-11ec-a7c5-1f95e3fcd1dd.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fcde3212-5952-11ec-a7ca-475810197ea8.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fcdf7b36-5952-11ec-a94e-23221e22a17d.jpg: 3 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fce8315e-5952-11ec-b271-3fc96b97409f.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fce839f6-5952-11ec-b27b-f75a579176d4.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fceb73f0-5952-11ec-b667-bb3981f05b17.jpg: 3 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fced045e-5952-11ec-b865-af1c521aac49.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fcee3e28-5952-11ec-b9c1-4bf2e7da1957.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fceff5ec-5952-11ec-bbce-074332da1af5.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fcf7ebb2-5952-11ec-bd65-d33708c031c5.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fcf896f2-5952-11ec-be2f-fb1d1f6c00bd.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fcf90740-5952-11ec-beb2-878a325d0ac1.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fcf9ae48-5952-11ec-bf8c-07cb1c60bf03.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fcfe73ec-5952-11ec-a1ad-4f362f37fd36.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fd000b08-5952-11ec-a3c7-7feaa124be04.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fd01a72e-5952-11ec-a5ae-97d9ce129500.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fd061b10-5952-11ec-aae5-ebe28cc4d72c.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fd0db8e8-5952-11ec-ac33-4f29a1fdc442.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/void/ml_oleg/annotations_s/images/val/fd0e526c-5952-11ec-acf2-7bbbb725e4ca.jpg: 1 duplicate labels removed\n",
      "\n",
      "                   all       1000       3578      0.607      0.431      0.489      0.339\n",
      "Speed: 0.7ms preprocess, 530.1ms inference, 0.0ms loss, 0.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "metrics_openvino = openvino_model.val(data='../test.yaml',batch=8,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a442b7c8-96d2-46df-beac-f69f14a51c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'metrics/precision(B)': 0.6066044770617985,\n",
       " 'metrics/recall(B)': 0.43096028971416317,\n",
       " 'metrics/mAP50(B)': 0.48948787092653767,\n",
       " 'metrics/mAP50-95(B)': 0.33889499193694395,\n",
       " 'fitness': 0.35395427983590333}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_openvino.results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e2875ff-40ec-4802-be4d-85f877b27f2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.timedelta(microseconds=374442)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_deepsparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c861451-b49e-4f84-869f-82c39a23f491",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading best_openvino_model for OpenVINO inference...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "datetime.timedelta(microseconds=536615)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_openvino = test_yolo(openvino_model)\n",
    "time_openvino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4695f7f6-3c6e-4fcb-88c9-6c476867031f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'metrics/precision(B)': 0.6066044770617985,\n",
       " 'metrics/recall(B)': 0.43096028971416317,\n",
       " 'metrics/mAP50(B)': 0.48948787092653767,\n",
       " 'metrics/mAP50-95(B)': 0.33889499193694395,\n",
       " 'fitness': 0.35395427983590333}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_openvino.results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f767cb-dcc6-4703-923e-dc8e834c5e21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.11",
   "language": "python",
   "name": "py3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
